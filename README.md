# Features

 Feature Optimization Techniques ‚Äî with Regression Focus
1Ô∏è‚É£ Feature Selection


Choosing a subset of the original features relevant to the target variable.



üîπ A. Filter Methods (Statistical; model-free)


| Method                | Use Case                                            | Regression? | Notes                                                   |
| --------------------- | --------------------------------------------------- | ----------- | ------------------------------------------------------- |
| VarianceThreshold     | Remove low-variance features                        | ‚úÖ           | Simple, preprocessing step                              |
| SelectKBest           | Score-based selection                               | ‚úÖ/‚ùå         | Use `f_regression` (‚úÖ) or `chi2` (‚ùå for classification) |
| SelectPercentile      | Top % features                                      | ‚úÖ/‚ùå         | Based on scoring function                               |
| Correlation Threshold | Drop weakly correlated or highly collinear features | ‚úÖ           | Based on Pearson/Spearman correlation                   |
| Mutual Information    | Non-linear dependency                               | ‚úÖ/‚ùå         | Use `mutual_info_regression`                            |
| ANOVA F-test          | Feature-class association                           | ‚ùå           | Categorical targets only                                |



üîπ B. Wrapper Methods (Model-based search)


| Method                              | Use Case                     | Regression? | Notes                             |
| ----------------------------------- | ---------------------------- | ----------- | --------------------------------- |
| RFE (Recursive Feature Elimination) | Recursive pruning            | ‚úÖ           | Needs a regressor (e.g., SVR, RF) |
| RFECV                               | RFE + cross-validation       | ‚úÖ           | Finds optimal #features           |
| Forward Selection                   | Adds one-by-one              | ‚úÖ           | Greedy, slow on large sets        |
| Backward Elimination                | Starts with all, removes     | ‚úÖ           | Inverse of forward                |
| SFS (Sequential FS)                 | Generalized forward/backward | ‚úÖ           | Can control scoring metric        |



üîπ C. Embedded Methods (Feature selection inside training)


| Method              | Use Case                         | Regression? | Notes                               |
| ------------------- | -------------------------------- | ----------- | ----------------------------------- |
| Lasso Regression    | Regularization-based             | ‚úÖ           | L1 penalty forces sparsity          |
| ElasticNet          | L1 + L2 penalty                  | ‚úÖ           | Better for correlated features      |
| Tree-based models   | Use splits to derive importances | ‚úÖ           | RF, XGBoost, LightGBM               |
| Ridge Regression    | L2 regularization                | ‚úÖ           | Coefficients shrink but not to zero |
| Logistic Regression | Classification only              | ‚ùå           | Not applicable for regression       |



üîπ D. Advanced Methods

| Method                 | Use Case                             | Regression? | Notes                                                             |
| ---------------------- | ------------------------------------ | ----------- | ----------------------------------------------------------------- |
| Boruta                 | All-relevant selection               | ‚úÖ/‚ùå         | Based on Random Forest                                            |
| SelectFromModel        | Uses model importance                | ‚úÖ           | Works with any estimator with `.coef_` or `.feature_importances_` |
| Genetic Algorithms     | Evolution-based search               | ‚úÖ           | Computationally expensive                                         |
| SHAP-based selection   | Use SHAP values to keep top features | ‚úÖ           | Model-agnostic                                                    |
| Permutation Importance | Drop-shuffle-test                    | ‚úÖ           | Interpretation-friendly                                           |




2Ô∏è‚É£ Feature Reduction

Compress features into a smaller transformed space.
üîπ A. Unsupervised Reduction

| Method       | Use Case                            | Regression? | Notes                      |
| ------------ | ----------------------------------- | ----------- | -------------------------- |
| PCA          | Projects to max variance directions | ‚úÖ           | Linear, fast               |
| Kernel PCA   | Non-linear PCA                      | ‚úÖ           | Expensive but powerful     |
| t-SNE        | Visualization                       | ‚ö†Ô∏è          | Use only for visualization |
| UMAP         | Visualization                       | ‚ö†Ô∏è          | Faster alt to t-SNE        |
| TruncatedSVD | Sparse PCA                          | ‚úÖ           | Faster for sparse matrices |
| ICA          | Independent signal separation       | ‚úÖ           | Signal processing focus    |

üîπ B. Supervised Reduction


| Method                                      | Use Case                    | Regression? | Notes                                      |
| ------------------------------------------- | --------------------------- | ----------- | ------------------------------------------ |
| **PLSR** (Partial Least Squares Regression) | Preserve X‚Äìy correlation    | ‚úÖ           | Powerful for few samples, many features    |
| LDA (Linear Discriminant Analysis)          | Maximize class separability | ‚ùå           | **Classification only**                    |
| Supervised PCA                              | PCA guided by target        | ‚úÖ           | Improves upon unsupervised PCA             |
| Autoencoders                                | Neural compression          | ‚úÖ           | Works for both regression & classification |

3Ô∏è‚É£ Feature Importance

| Method                       | Use Case                     | Regression? | Notes                              |
| ---------------------------- | ---------------------------- | ----------- | ---------------------------------- |
| Coefficients (Linear Models) | Sign & magnitude matter      | ‚úÖ           | Standard for linear models         |
| Tree-based Importances       | Based on splits              | ‚úÖ           | `.feature_importances_`            |
| Permutation Importance       | Shuffle-drop-test            | ‚úÖ           | Works on any model                 |
| **SHAP Values**              | Local + global importance    | ‚úÖ           | Model-agnostic, most interpretable |
| LIME                         | Local explanations           | ‚úÖ           | For individual predictions         |
| Drop Column Importance       | Retrain without one feature  | ‚úÖ           | Simple but slow                    |
| Partial Dependence Plots     | Feature effect visualization | ‚úÖ           | Visual tool for interpretation     |




‚úÖ Summary Flow (Which to Use When)

    If target = continuous ‚Üí You are in regression

    ‚Üí Use PLSR, PCA, SelectKBest (f_regression), SHAP, Permutation, Autoencoders, etc.

    If target = categorical ‚Üí You are in classification

    ‚Üí Use LDA, chi2, ANOVA F-test, Logistic Regression coefficients, etc.
