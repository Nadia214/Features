# Features

 Feature Optimization Techniques â€” with Regression Focus
1ï¸âƒ£ Feature Selection

Choosing a subset of the original features relevant to the target variable.
ğŸ”¹ A. Filter Methods (Statistical; model-free)
Method	Use Case	Regression?	Notes
VarianceThreshold	Remove low-variance features	âœ…	Simple, preprocessing step
SelectKBest	Score-based selection	âœ…/âŒ	Use f_regression (âœ…) or chi2 (âŒ for classification)
SelectPercentile	Top % features	âœ…/âŒ	Based on scoring function
Correlation Threshold	Drop weakly correlated or highly collinear features	âœ…	Based on Pearson/Spearman correlation
Mutual Information	Non-linear dependency	âœ…/âŒ	Use mutual_info_regression
ANOVA F-test	Feature-class association	âŒ	Categorical targets only
ğŸ”¹ B. Wrapper Methods (Model-based search)
Method	Use Case	Regression?	Notes
RFE (Recursive Feature Elimination)	Recursive pruning	âœ…	Needs a regressor (e.g., SVR, RF)
RFECV	RFE + cross-validation	âœ…	Finds optimal #features
Forward Selection	Adds one-by-one	âœ…	Greedy, slow on large sets
Backward Elimination	Starts with all, removes	âœ…	Inverse of forward
SFS (Sequential FS)	Generalized forward/backward	âœ…	Can control scoring metric
ğŸ”¹ C. Embedded Methods (Feature selection inside training)
Method	Use Case	Regression?	Notes
Lasso Regression	Regularization-based	âœ…	L1 penalty forces sparsity
ElasticNet	L1 + L2 penalty	âœ…	Better for correlated features
Tree-based models	Use splits to derive importances	âœ…	RF, XGBoost, LightGBM
Ridge Regression	L2 regularization	âœ…	Coefficients shrink but not to zero
Logistic Regression	Classification only	âŒ	Not applicable for regression
ğŸ”¹ D. Advanced Methods
Method	Use Case	Regression?	Notes
Boruta	All-relevant selection	âœ…/âŒ	Based on Random Forest
SelectFromModel	Uses model importance	âœ…	Works with any estimator with .coef_ or .feature_importances_
Genetic Algorithms	Evolution-based search	âœ…	Computationally expensive
SHAP-based selection	Use SHAP values to keep top features	âœ…	Model-agnostic
Permutation Importance	Drop-shuffle-test	âœ…	Interpretation-friendly
2ï¸âƒ£ Feature Reduction

Compress features into a smaller transformed space.
ğŸ”¹ A. Unsupervised Reduction
Method	Use Case	Regression?	Notes
PCA	Projects to max variance directions	âœ…	Linear, fast
Kernel PCA	Non-linear PCA	âœ…	Expensive but powerful
t-SNE	Visualization	âš ï¸	Use only for visualization
UMAP	Visualization	âš ï¸	Faster alt to t-SNE
TruncatedSVD	Sparse PCA	âœ…	Faster for sparse matrices
ICA	Independent signal separation	âœ…	Signal processing focus
ğŸ”¹ B. Supervised Reduction
Method	Use Case	Regression?	Notes
PLSR (Partial Least Squares Regression)	Preserve Xâ€“y correlation	âœ…	Powerful for few samples, many features
LDA (Linear Discriminant Analysis)	Maximize class separability	âŒ	Classification only
Supervised PCA	PCA guided by target	âœ…	Improves upon unsupervised PCA
Autoencoders	Neural compression	âœ…	Works for both regression & classification
3ï¸âƒ£ Feature Importance

Measure how much each feature contributes to predictions.
Method	Use Case	Regression?	Notes
Coefficients (Linear Models)	Sign & magnitude matter	âœ…	Standard for linear models
Tree-based Importances	Based on splits	âœ…	.feature_importances_
Permutation Importance	Shuffle-drop-test	âœ…	Works on any model
SHAP Values	Local + global importance	âœ…	Model-agnostic, most interpretable
LIME	Local explanations	âœ…	For individual predictions
Drop Column Importance	Retrain without one feature	âœ…	Simple but slow
Partial Dependence Plots	Feature effect visualization	âœ…	Visual tool for interpretation
âœ… Summary Flow (Which to Use When)

    If target = continuous â†’ You are in regression

    â†’ Use PLSR, PCA, SelectKBest (f_regression), SHAP, Permutation, Autoencoders, etc.

    If target = categorical â†’ You are in classification

    â†’ Use LDA, chi2, ANOVA F-test, Logistic Regression coefficients, etc.
